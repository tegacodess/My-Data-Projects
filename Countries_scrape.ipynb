{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObMT8zd1arXbtRycG3AJ74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tegacodess/My-Data-Projects/blob/main/Countries_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Country Data Webscrape Activity\n",
        "\n",
        "In this notebook, I practiced my newly gained knowledge on webscraping. The goal is to get the data on the Countries of the world from the  [Scrape This Site sandbox](https://www.scrapethissite.com/pages/simple/ ).\n",
        "\n",
        "At  the end of this, I would store the scraped data in a pandas dataframe, and then convert the dataframe into formats CSV, TSV, and Excel.   \n",
        "\n",
        "#### Tools Used\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   Pandas Library: For Data Structuring\n",
        "* BeautifulSoup Library: To parse HTML content\n",
        "* Request Library:  to Handle HTTP requests\n",
        "\n"
      ],
      "metadata": {
        "id": "Op6l-66GQtO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnpmWhypJ6Bs"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the url of the site i would be scraping in a variable and ensuring it works\n",
        "url = 'https://www.scrapethissite.com/pages/simple/'\n",
        "url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "I6jl-aEsKUG9",
        "outputId": "610e386f-4a78-4671-9e60-ab5979eefe9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.scrapethissite.com/pages/simple/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# send a http get request to the site and show its status code\n",
        "response = requests.get(url)\n",
        "response.status_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcKzMaH2KNnH",
        "outputId": "d568f192-7c94-40b4-b8d9-92082aa4dc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing the HTML content of the page\n",
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "ei6JWK6mKJXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape Data\n",
        "\n",
        "#### Checklist\n",
        "The following data will be obtained and stored in a list:\n",
        "\n",
        "*   Country name\n",
        "*   Country capital\n",
        "* Country population\n",
        "* Area of the country ( $km^2$ )\n",
        "\n",
        "#### Method Used\n",
        "* Find all elements of a particular category (e.g country name) using its html class name and element.\n",
        "* Create a list to store the data\n",
        "* Using a for loop, strip the html code from each iter and append it to the created list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4DWMcAwaoegf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Country Name\n"
      ],
      "metadata": {
        "id": "YrOAgjtAPAdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country = soup.find_all('h3', class_ = 'country-name')\n",
        "\n",
        "country_name =[]\n",
        "\n",
        "# for loop to iterate over the scraped data containing the html code, extract the text and store in created list (country_name)\n",
        "for name in country:\n",
        "  name_ = name.get_text(strip=True)\n",
        "  country_name.append(name_)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TGIC_97yN4oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  2. Country Capital"
      ],
      "metadata": {
        "id": "XQ7SekOoP616"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_capital = soup.find_all('span', class_='country-capital')\n",
        "\n",
        "capital =[]\n",
        "\n",
        "for cap in country_capital:\n",
        "  cap_ = cap.get_text(strip=True)\n",
        "  capital.append(cap_)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Qxs-Us9PwKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Country Population\n",
        "\n"
      ],
      "metadata": {
        "id": "pi1WerBNZ4kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_population = soup.find_all('span', class_='country-population')\n",
        "\n",
        "population =[]\n",
        "\n",
        "for pop in country_population:\n",
        "  pop_ = pop.get_text(strip=True)\n",
        "  population.append(pop_)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "au-P6-aLQISb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Country Area"
      ],
      "metadata": {
        "id": "ONESHqf6aWJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country_area = soup.find_all('span', class_='country-area')\n",
        "area= []\n",
        "for aa in country_area:\n",
        "  aa_ = aa.get_text(strip=True)\n",
        "  area.append(aa_)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bmcO6pFpaSMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a dataframe with the curated lists."
      ],
      "metadata": {
        "id": "gGQeEXSMa8w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "data['Country'] = country_name\n",
        "data['Capital'] = capital\n",
        "data['Population'] = population\n",
        "data[\"Area (km\\u00b2)\"] = area\n",
        "\n"
      ],
      "metadata": {
        "id": "jguUKwNIarnN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('Countries WebScrape', index= False)"
      ],
      "metadata": {
        "id": "9xvdnevQlRLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same logic, but refactored and shorter code length\n",
        "\n",
        "url =  'https://www.scrapethissite.com/pages/simple/'\n",
        "response = requests.get(url)\n",
        "# response.status_code\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Scraping data\n",
        "Countries = []\n",
        "country = soup.find_all('div', class_ = 'col-md-4 country' )\n",
        "\n",
        "# For loop to get data and append it in the 'Countries' list\n",
        "for count in country:\n",
        "  name = count.find('h3', class_ = 'country-name').get_text(strip = True)\n",
        "  capital =count.find('span', class_ = 'country-capital').get_text(strip = True)\n",
        "  population = count.find('span', class_ = 'country-population').get_text(strip = True)\n",
        "  area =  count.find('span', class_ = 'country-area').get_text(strip = True)\n",
        "\n",
        "  Countries.append({\n",
        "      'Name' :name,\n",
        "      'Capital': capital,\n",
        "      'Population': population,\n",
        "      'Area (km\\u00b2)' : area\n",
        "  })\n",
        "\n",
        "# Conversion to DataFrame\n",
        "Countries_Data=pd.DataFrame(Countries)\n"
      ],
      "metadata": {
        "id": "X1r3NwfLi27b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export CSV, TSV and Excel respectively\n",
        "Countries_Data.to_csv('Scraped Countries Data.csv', index=True)\n",
        "Countries_Data.to_csv('Scraped Countries Data.tsv', sep ='\\t', index=True)\n",
        "Countries_Data.to_excel('Scraped Countries Data.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "BtMk5WFId3d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion\n",
        "\n",
        "The data contained in the Checklist; country name, capital, population and area, were succesfully obtained from the site and stored in a pandas dataframe which was then converted into three file formats csv, tsv and excel."
      ],
      "metadata": {
        "id": "vK7QNNHA4mKC"
      }
    }
  ]
}